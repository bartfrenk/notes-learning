#+TITLE: Notes on Mastering PostgreSQL 9.6
#+AUTHOR: Bart Frenk
#+TAGS: interesting

* Meta
  Description: Notes on Schonig - Mastering PostgreSQL 9.6 (2017)
* Contents
  :PROPERTIES:
  :engine:   postgresql
  :cmdline: "-h localhost -p 15432 -U docker"
  :exports: both
  :visibility: children
  :END:

** DONE Preliminaries
   CLOSED: [2018-02-07 Wed 23:10]
*** Build a docker image with PostgreSQL and curl
Create the temporary directory:
#+BEGIN_SRC sh
mkdir -p /tmp/schonig-mastering-postgresql-9-6
#+END_SRC

#+RESULTS:

Use C-c C-v t to create the Dockerfile:
#+BEGIN_SRC dockerfile :exports code :padline no :tangle /tmp/schonig-mastering-postgresql-9-6/Dockerfile
FROM postgres:9.6.2
RUN apt-get update && apt-get install -y curl
#+END_SRC

Build the docker image. This might take a while.
#+BEGIN_SRC sh
docker build -t schonig-mastering-postgresql-9-6 /tmp/schonig-mastering-postgresql-9-6/
#+END_SRC

*** Create and start a docker container

Create and run a docker container:
#+BEGIN_SRC sh
docker run -d --name mastering-postgresql-db \
       -p 15432:5432 \
       -e POSTGRES_USER=docker \
       -e POSTGRES_DB=docker \
       schonig-mastering-postgresql-9-6
#+END_SRC

#+RESULTS:
: 5056b3a4e1c9fc377c697808327fbb3fef35d2bf88189079d2b758c8b9123551

Start Docker container:
#+BEGIN_SRC sh
docker start mastering-postgresql-db
#+END_SRC

#+RESULTS:
: mastering-postgres-db

*** Seed the database

Create table and load data:
#+BEGIN_SRC sql
CREATE TABLE t_oil (
       region text,
       country text,
       year int,
       production int,
       consumption int
       );

COPY t_oil FROM PROGRAM 'curl -L www.cybertec.at/secret/oil_ext.txt ';
#+END_SRC

   #+RESULTS:
   | CREATE TABLE |
   |--------------|
   | COPY 644     |

** PostgreSQL overview
   #+BEGIN_SRC sql
   SHOW max_worker_processes
   #+END_SRC

   #+RESULTS:
   | max_worker_processes |
   |----------------------|
   |                    8 |

** DONE Understanding transactions and locking
   CLOSED: [2018-03-17 Sat 23:37]
*** Working with PostgreSQL transactions
    now() returns transaction time

   #+BEGIN_SRC sql
   SELECT now(), now()
   #+END_SRC

   #+RESULTS:
   | now                         | now                         |
   |-----------------------------+-----------------------------|
   | 2018-03-13 22:17:06.6095+00 | 2018-03-13 22:17:06.6095+00 |

   #+BEGIN_SRC sql
   SELECT now();
   SELECT now()
   #+END_SRC

   #+RESULTS:
   | now                           |
   |-------------------------------|
   | 2018-03-13 22:19:25.857049+00 |
   | now                           |
   | 2018-03-13 22:19:25.857415+00 |

   For multiple statements to return the transaction time, they need to be
   started with a BEGIN statement.
   
   #+BEGIN_SRC sql
   BEGIN;
   SELECT now();
   SELECT now();
   COMMIT
   #+END_SRC

   #+RESULTS:
   | BEGIN                         |
   |-------------------------------|
   | now                           |
   | 2018-03-13 22:19:42.807131+00 |
   | now                           |
   | 2018-03-13 22:19:42.807131+00 |
   | COMMIT                        |

**** Handling errors inside a transaction
     Only error-free transactions can be committed
**** Making use of savepoints
     #+BEGIN_SRC sql
     BEGIN;
     SELECT 1;
     SAVEPOINT a;
     SELECT 2 / 0;
     ROLLBACK TO SAVEPOINT a;
     SELECT 3;
     COMMIT;
     #+END_SRC

     #+RESULTS:
     | BEGIN     |
     |-----------|
     | ?column?  |
     | 1         |
     | SAVEPOINT |
     | ROLLBACK  |
     | ?column?  |
     | 3         |
     | COMMIT    |

     The number of savepoints inside a transaction is practically unlimited.

     Point in a transaction to rollback to
**** Transactional DDLs
     
     All DDLs in PostgreSQL are transactional except:
     - DROP DATABASE
     - CREATE TABLESPACE/DROP TABLESPACE on so on
     
     #+BEGIN_SRC sql
     BEGIN;
     CREATE TABLE t_test (id int);
     ALTER TABLE t_test ALTER COLUMN id TYPE int8;
     COMMIT
     #+END_SRC

     #+RESULTS:
     | BEGIN        |
     |--------------|
     | CREATE TABLE |
     | ALTER TABLE  |
     | COMMIT       |
                   
     #+BEGIN_SRC sql
     SELECT *
     FROM pg_stat_user_tables;
     #+END_SRC

     #+RESULTS:
     | relid | schemaname | relname | seq_scan | seq_tup_read | idx_scan | idx_tup_fetch | n_tup_ins | n_tup_upd | n_tup_del | n_tup_hot_upd | n_live_tup | n_dead_tup | n_mod_since_analyze | last_vacuum | last_autovacuum | last_analyze | last_autoanalyze | vacuum_count | autovacuum_count | analyze_count | autoanalyze_count |
     |-------+------------+---------+----------+--------------+----------+---------------+-----------+-----------+-----------+---------------+------------+------------+---------------------+-------------+-----------------+--------------+------------------+--------------+------------------+---------------+-------------------|

*** Understanding basic locking
    #+BEGIN_SRC sql
    DROP TABLE t_test
    #+END_SRC

    #+RESULTS:
    | DROP TABLE |
    |------------|


    #+BEGIN_SRC sql
    CREATE TABLE t_test (id int);
    INSERT INTO t_test VALUES (1);
    #+END_SRC

    #+RESULTS:
    | CREATE TABLE |
    |--------------|
    | INSERT 0 1   |

   #+BEGIN_SRC sql
   BEGIN;
   UPDATE t_test SET id = id + 1 RETURNING *;
   COMMIT
#+END_SRC

   #+RESULTS:
   | BEGIN    |
   |----------|
   | id       |
   | 2        |
   | UPDATE 1 |
   | COMMIT   |


   - A transaction can see only those changes that have already been committed
   - Writing transactions will not block reading transactions
   - PostgreSQL will only lock rows affected by the UPDATE
**** Avoiding typical mistakes and explicit locking
*** Making use of for share and for update
    SELECT .. FOR UPDATE block each other; this allows the application to do
    read-modify-write cycles correctly. There is also SELECT .. FOR UPDATE SKIP
    LOCKED, which is not blocked, but only returns rows for which no lock is
    active.
*** Understanding transaction isolation levels
**** Phenomena defined in the SQL standard
***** dirty read
      Read a value that has not been committed yet
***** nonrepeatable read
      Reading data in a transaction twice yields different values
***** phantom read
      Selections change during transaction
***** dirty write
      Overwrite uncommitted value

**** Isolation levels (SQL)
***** READ UNCOMMITTED
      Not possible in PostgreSQL, silently mapped to READ COMMITTED.
***** READ COMMITTED
      Every statement inside a transaction will get a new snapshot of the
      data. This is the default isolation level.
***** REPEATABLE READ
      Transaction will use the same snapshot throughout the entire
      transaction. This isolation level is not more costly than READ COMMITTED.
***** SERIALIZABLE
      Transactions performed as the would be by a single client (in some order
      matching the time frames of the transactions).
*** Observing deadlocks and similar issues
    deadlocks will be resolved after the duration set in =deadlock_timeout=.
*** Utilizing advisory locks
    PostgreSQL has a function to unlock all advisory locks, =pg_advisory_unlock_all()=
*** Optimizing storage and managing cleanup
**** Configuring vacuum and autovacuum    
**** Watching vacuum at work                                    :interesting:
     Example of table size and vacuum.

     To see human-readable description of the size of 't_test'.
     #+BEGIN_SRC sql
     SELECT pg_size_pretty(pg_relation_size('t_test'));
     #+END_SRC

** STARTED Making use of indexes
   After 17 years of professional, full-time PostgreSQL consulting and
   PostgreSQL 24x7 support, I can say one thing for sure. Bad indexing is the
   main source of bad performance.  Of course, it is important to adjust memory
   parameters and all that. However, it is all in vain if indexes are not used
   properly. There is simply no replacement for a missing index. (p.43)
*** Understanding simple queries and the cost model

#+BEGIN_SRC sql
CREATE TABLE t_test (id serial, name text);
INSERT INTO t_test (name) SELECT 'hans'
FROM generate_series(1, 2000000);
INSERT INTO t_test (name) SELECT 'paul'
FROM generate_series(1, 2000000);
#+END_SRC

#+RESULTS:
| CREATE TABLE     |
|------------------|
| INSERT 0 2000000 |
| INSERT 0 2000000 |

#+RESULTS:

#+BEGIN_SRC sql
SELECT name, count(*) FROM t_test GROUP BY 1;
#+END_SRC

#+RESULTS:
| name |   count |
|------+---------|
| hans | 2000000 |
| paul | 2000000 |

#+BEGIN_SRC sql
ANALYZE t_test;
#+END_SRC

#+RESULTS:
| ANALYZE |
|---------|


#+BEGIN_SRC sql
EXPLAIN SELECT * FROM t_test WHERE id = 432332;
#+END_SRC

#+RESULTS:
| QUERY PLAN                                               |
|----------------------------------------------------------|
| Seq Scan on t_test  (cost=0.00..71622.00 rows=1 width=9) |
| Filter: (id = 432332)                                    |

#+BEGIN_SRC sql
SELECT pg_relation_size('t_test') / 8192.0;
#+END_SRC

#+RESULTS:
|           ?column? |
|--------------------|
| 21622.000000000000 |

#+BEGIN_SRC sql
SHOW seq_page_cost;
SHOW cpu_tuple_cost;
SHOW cpu_operator_cost;
SHOW random_page_cost;
SHOW cpu_index_tuple_cost;
SHOW parallel_tuple_cost;
SHOW parallel_setup_cost;
SHOW min_parallel_relation_size;
#+END_SRC

#+RESULTS:
| seq_page_cost              |
|----------------------------|
| 1                          |
| cpu_tuple_cost             |
| 0.01                       |
| cpu_operator_cost          |
| 0.0025                     |
| random_page_cost           |
| 4                          |
| cpu_index_tuple_cost       |
| 0.005                      |
| parallel_tuple_cost        |
| 0.1                        |
| parallel_setup_cost        |
| 1000                       |
| min_parallel_relation_size |
| 8MB                        |

#+BEGIN_SRC python :session :exports code
21622 * 1 + 4000000 * 0.01 + 4000000 * 0.0025
#+END_SRC

#+RESULTS:
: 71622.0

#+BEGIN_SRC sql
CREATE INDEX idx_id ON t_test (id);
#+END_SRC

#+RESULTS:
| CREATE INDEX |
|--------------|

#+BEGIN_SRC sql
EXPLAIN SELECT * FROM t_test WHERE id = 432332;
#+END_SRC

#+RESULTS:
| QUERY PLAN                                                          |
|---------------------------------------------------------------------|
| Index Scan using idx_id on t_test  (cost=0.43..8.45 rows=1 width=9) |
| Index Cond: (id = 432332)                                           |

#+BEGIN_SRC sql
SELECT relname,
       reltuples,
       pg_size_pretty(relpages::bigint * 8 * 1024) AS size
FROM pg_class
WHERE relname = 'idx_id'
ORDER BY relpages DESC;
#+END_SRC

#+RESULTS:
| relname | reltuples | size  |
|---------+-----------+-------|
| idx_id  |     4e+06 | 86 MB |

B-tree indexes are not only useful to find rows. They are also useful to feed
sorted data to the next stage in the process:

#+BEGIN_SRC sql
EXPLAIN SELECT * FROM t_test ORDER BY id DESC LIMIT 10;
#+END_SRC

#+RESULTS:
| QUERY PLAN                                                                                  |
|---------------------------------------------------------------------------------------------|
| Limit  (cost=0.43..125505.43 rows=4000000 width=9)                                          |
| ->  Index Scan Backward using idx_id on t_test  (cost=0.43..125505.43 rows=4000000 width=9) |



**** WAIT Read the article on concurrent B-trees
See Lehman, Yao
*** Taking advantage of pg_trgm
**** Install extension

#+BEGIN_SRC sql
CREATE EXTENSION pg_trgm
#+END_SRC

#+RESULTS:
| CREATE EXTENSION |
|------------------|

**** Seed database

#+BEGIN_SRC sql
CREATE TABLE t_location (name text)
#+END_SRC

#+RESULTS:
| CREATE TABLE |
|--------------|

#+BEGIN_SRC sql
COPY t_location FROM PROGRAM 'curl -L www.cybertec.at/secret/orte.txt'
#+END_SRC

#+RESULTS:
| COPY 2354 |
|-----------|

**** Experiment with pg_trgm

#+BEGIN_SRC sql
SELECT 'abcde' <-> 'abdeacb'
#+END_SRC

#+RESULTS:
| ?column? |
|----------|
| 0.833333 |

#+NAME: trigrams-abcde
#+BEGIN_SRC sql
SELECT show_trgm('abcde')
#+END_SRC

#+RESULTS:
| show_trgm                       |
|---------------------------------|
| {"  a"," ab",abc,bcd,cde,"de "} |

#+NAME: trigrams-abdeacb
#+BEGIN_SRC sql
SELECT show_trgm('abdeacb')
#+END_SRC

#+RESULTS:
| show_trgm                               |
|-----------------------------------------|
| {"  a"," ab",abd,acb,bde,"cb ",dea,eac} |

#+BEGIN_SRC emacs-lisp :var table=trigrams-abcde[2, 0]
(print table)
#+END_SRC

#+RESULTS:
: {"  a"," ab",abc,bcd,cde,"de "}

#+BEGIN_SRC sql
SELECT * FROM t_location ORDER BY name <-> 'Kramertneusiedel' LIMIT 3;
#+END_SRC

#+RESULTS:
| name           |
|----------------|
| Gramatneusiedl |
| Klein-Neusiedl |
| Potzneusiedl   |

**** Compare pg_trgm GiST index on a small table

#+BEGIN_SRC sql
SELECT count(*) FROM t_location;
#+END_SRC

#+RESULTS:
| count |
|-------|
|  2354 |

Create B-tree index on name to compare with GiST index:
#+BEGIN_SRC sql
CREATE INDEX idx_name ON t_location (name)
#+END_SRC

#+RESULTS:
| CREATE INDEX |
|--------------|

Compare index and relation sizes:
#+BEGIN_SRC sql
SELECT pg_size_pretty(pg_relation_size('idx_trgm')) as idx_trgm,
       pg_size_pretty(pg_relation_size('idx_name')) as idx_name,
       pg_size_pretty(pg_relation_size('t_location')) as t_location
#+END_SRC

#+RESULTS:
| idx_trgm | idx_name | t_location |
|----------+----------+------------|
| 200 kB   | 96 kB    | 112 kB     |

#+NAME: with-idx_trgm
#+BEGIN_SRC sql
CREATE INDEX idx_trgm ON t_location USING GiST(name GiST_trgm_ops);
EXPLAIN SELECT * FROM t_location ORDER BY name <-> 'Kramertneusiedel' LIMIT 3;
#+END_SRC

#+NAME: without-idx_trgm
#+BEGIN_SRC sql
DROP INDEX IF EXISTS idx_trgm; 
EXPLAIN SELECT * FROM t_location ORDER BY name <-> 'Kramertneusiedel' LIMIT 3;
#+END_SRC

#+NAME: exact-name
#+BEGIN_SRC sql
EXPLAIN SELECT * FROM t_location WHERE name = 'Gramatneusiedl';
#+END_SRC

#+RESULTS: with-idx_trgm
| CREATE INDEX                                                                        |
|-------------------------------------------------------------------------------------|
| QUERY PLAN                                                                          |
| Limit  (cost=0.14..0.40 rows=3 width=17)                                            |
| ->  Index Scan using idx_trgm on t_location  (cost=0.14..203.22 rows=2354 width=17) |
| Order By: (name <-> 'Kramertneusiedel'::text)                                       |

#+RESULTS: without-idx_trgm
| DROP INDEX                                                        |
|-------------------------------------------------------------------|
| QUERY PLAN                                                        |
| Limit  (cost=73.85..73.86 rows=3 width=17)                        |
| ->  Sort  (cost=73.85..79.74 rows=2354 width=17)                  |
| Sort Key: ((name <-> 'Kramertneusiedel'::text))                   |
| ->  Seq Scan on t_location  (cost=0.00..43.42 rows=2354 width=17) |

#+RESULTS: exact-name
| QUERY PLAN                                                                      |
|---------------------------------------------------------------------------------|
| Index Only Scan using idx_name on t_location  (cost=0.28..8.30 rows=1 width=13) |
| Index Cond: (name = 'Gramatneusiedl'::text)                                     |

**** Compare trigram GiST index on larger table

***** Create function to generate random string

Create a function to generate random strings. From this [[http://www.simononsoftware.com/random-string-in-postgresql/][blog post]].
#+BEGIN_SRC sql
CREATE OR REPLACE FUNCTION random_text(length INTEGER)
RETURNS TEXT
LANGUAGE PLPGSQL
AS $$ 
DECLARE 
  cs TEXT := concat('0123456789',
                    'ABCDEFGHIJKLMNOPQRSTUVWXYZ',
                    'abcdefghijklmnopqrstuvwxyz'); 
  res TEXT := '';
  i INT4;
  n INTEGER; 
BEGIN
  n := length(cs);
  FOR i IN 1..length LOOP 
    res := res || substr(cs, (1 + FLOOR((n + 1) * random() ))::INTEGER, 1);
  END LOOP;
  RETURN res;
END; $$;
#+END_SRC

#+RESULTS:
| CREATE FUNCTION |
|-----------------|

#+BEGIN_SRC sql
SELECT random_text(10);
#+END_SRC

#+RESULTS:
| random_text |
|-------------|
| lF7jhITRpd  |

***** Set up database

Create a table with a single bounded text column:

#+BEGIN_SRC sql
DROP TABLE IF EXISTS t_names;
CREATE TABLE t_names (name varchar(255))
#+END_SRC

#+RESULTS:
| DROP TABLE   |
|--------------|
| CREATE TABLE |

Create a trigram-based GiST index:

#+BEGIN_SRC sql
CREATE INDEX idx_name_trgm ON t_names USING GiST(name GiST_trgm_ops)
#+END_SRC

#+RESULTS:
| CREATE INDEX |
|--------------|

Create a B-tree index for comparison:

#+BEGIN_SRC sql
CREATE INDEX idx_name_btree ON t_names (name)
#+END_SRC

#+RESULTS:
| CREATE INDEX |
|--------------|

***** Perform the experiment

List the sizes of all object under consideration:

#+BEGIN_SRC sql
SELECT pg_size_pretty(pg_relation_size('idx_name_trgm')) as idx_name_trgm,
       pg_size_pretty(pg_relation_size('t_names')) as t_names,
       pg_size_pretty(pg_relation_size('idx_name_btree')) as idx_name_btree;
#+END_SRC

#+RESULTS:
| idx_name_trgm | t_names | idx_name_btree |
|---------------+---------+----------------|
| 83 MB         | 29 MB   | 32 MB          |

Insert 100K random strings of length 42 in the table, and return the total count:

#+BEGIN_SRC sql
INSERT INTO t_names
SELECT random_text(42)
FROM generate_series(1, 100000);
SELECT COUNT(*) FROM t_names;
#+END_SRC

#+RESULTS:
| INSERT 0 100000 |
|-----------------|
| count           |
| 400000          |

Select the closest words:

#+BEGIN_SRC sql
EXPLAIN SELECT name FROM t_names ORDER BY 'HelloWorld' <-> name LIMIT 50;
#+END_SRC

#+RESULTS:
| QUERY PLAN                                                                                |
|-------------------------------------------------------------------------------------------|
| Limit  (cost=0.41..8.60 rows=50 width=46)                                                 |
| ->  Index Scan using idx_name_trgm on t_names  (cost=0.41..65564.41 rows=400000 width=46) |
| Order By: ((name)::text <-> 'HelloWorld'::text)                                           |

Select a precise word:

#+BEGIN_SRC sql
EXPLAIN SELECT name FROM t_names WHERE name = 'HelloWorld';
#+END_SRC

#+RESULTS:
| QUERY PLAN                                                                         |
|------------------------------------------------------------------------------------|
| Index Only Scan using idx_name_btree on t_names  (cost=0.42..8.44 rows=1 width=42) |
| Index Cond: (name = 'HelloWorld'::text)                                            |

***** Conclusion
Selecting the $n$ closest words gets to be more expensive when $n$ is around 50,
when there are 400K random words in the database of length 42. Here, random
means that each letter of the word is selected uniformly at random from the set
of digits, upper-case and lower-case letters.


** DONE Handling advanced SQL
   CLOSED: [2018-02-07 Wed 23:09]
*** GROUPING SETS, CUBE, ROLLUP
   
    #+BEGIN_SRC sql
    SELECT region, country, avg(production)
    FROM t_oil
    WHERE country IN ('USA', 'Canada', 'Iran', 'Oman')
    GROUP BY CUBE (region, country)
    #+END_SRC

    #+RESULTS:

    #+BEGIN_SRC sql
    EXPLAIN ANALYZE SELECT region, sum(production)
    FROM t_oil
    GROUP BY region
    #+END_SRC

    #+RESULTS:
    | QUERY PLAN                                                                                              |
    |---------------------------------------------------------------------------------------------------------|
    | HashAggregate  (cost=15.66..15.68 rows=2 width=16) (actual time=0.200..0.200 rows=2 loops=1)            |
    | Group Key: region                                                                                       |
    | ->  Seq Scan on t_oil  (cost=0.00..12.44 rows=644 width=16) (actual time=0.003..0.051 rows=644 loops=1) |
    | Planning time: 0.175 ms                                                                                 |
    | Execution time: 0.247 ms                                                                                |

    #+BEGIN_SRC sql
    EXPLAIN ANALYZE SELECT region, country, sum(production)
    FROM t_oil
    GROUP BY ROLLUP (region, country)
    #+END_SRC

    #+RESULTS:
    | QUERY PLAN                                                                                              |
    |---------------------------------------------------------------------------------------------------------|
    | GroupAggregate  (cost=42.49..49.24 rows=31 width=24) (actual time=0.392..0.522 rows=17 loops=1)         |
    | Group Key: region, country                                                                              |
    | Group Key: region                                                                                       |
    | Group Key: ()                                                                                           |
    | ->  Sort  (cost=42.49..44.10 rows=644 width=24) (actual time=0.377..0.401 rows=644 loops=1)             |
    | Sort Key: region, country                                                                               |
    | Sort Method: quicksort  Memory: 75kB                                                                    |
    | ->  Seq Scan on t_oil  (cost=0.00..12.44 rows=644 width=24) (actual time=0.004..0.085 rows=644 loops=1) |
    | Planning time: 0.180 ms                                                                                 |
    | Execution time: 0.570 ms                                                                                |

    #+BEGIN_SRC sql
    EXPLAIN (ANALYZE, TIMING, BUFFERS, COSTS) SELECT sum(production)
    FROM t_oil
    #+END_SRC

    #+RESULTS:
    | QUERY PLAN                                                                                             |
    |--------------------------------------------------------------------------------------------------------|
    | Aggregate  (cost=14.05..14.06 rows=1 width=4) (actual time=0.118..0.118 rows=1 loops=1)                |
    | Buffers: shared hit=6                                                                                  |
    | ->  Seq Scan on t_oil  (cost=0.00..12.44 rows=644 width=4) (actual time=0.007..0.044 rows=644 loops=1) |
    | Buffers: shared hit=6                                                                                  |
    | Planning time: 0.124 ms                                                                                |
    | Execution time: 0.150 ms                                                                               |

    #+BEGIN_SRC sql
    CREATE INDEX country_idx
    ON t_oil (country)
    #+END_SRC

    #+RESULTS:
    | CREATE INDEX |
    |--------------|

    #+BEGIN_SRC sql
    DROP INDEX country_idx;
    #+END_SRC

    #+RESULTS:
    | DROP INDEX |
    |------------|


    #+BEGIN_SRC sql
    EXPLAIN SELECT region, country, sum(production) as production
    FROM t_oil
    GROUP BY GROUPING SETS ((), region, country);
    #+END_SRC

    #+RESULTS:
    | QUERY PLAN                                                  |
    |-------------------------------------------------------------|
    | GroupAggregate  (cost=42.49..82.53 rows=17 width=24)        |
    | Group Key: region                                           |
    | Group Key: ()                                               |
    | Sort Key: country                                           |
    | Group Key: country                                          |
    | ->  Sort  (cost=42.49..44.10 rows=644 width=24)             |
    | Sort Key: region                                            |
    | ->  Seq Scan on t_oil  (cost=0.00..12.44 rows=644 width=24) |

*** FILTER

    #+BEGIN_SRC sql
    SELECT
        region,
        sum(production) AS total,
        sum(production) FILTER (WHERE year < 1990) AS old,
        sum(production) FILTER (WHERE year >= 1990) AS new
    FROM t_oil
    GROUP BY ROLLUP (region)
    #+END_SRC

    #+RESULTS:
    | region        |   total |    old |    new |
    |---------------+---------+--------+--------|
    | Middle East   |  864790 | 391401 | 473389 |
    | North America |  626708 | 335374 | 291334 |
    |               | 1491498 | 726775 | 764723 |


    Note that if it is possible to move conditions to a WHERE clause it is
    always more desirable as less data has to be fetched from the table. FILTER
    is only useful if the data left by the WHERE clause is not needed by each
    aggregate. (p.96)


*** Making use of ordered sets: mode, percentile_disc, percentile_cont
    
    #+BEGIN_SRC sql
    SELECT region,
           percentile_disc(0.5) WITHIN GROUP (ORDER BY production) AS median,
           percentile_cont(0.5) WITHIN GROUP (ORDER BY production) AS interpolated
    FROM t_oil
    GROUP BY 1;
    #+END_SRC

    #+RESULTS:
    | region        | median | interpolated |
    |---------------+--------+--------------|
    | Middle East   |   1082 |         1094 |
    | North America |   3054 |       3066.5 |

    #+BEGIN_SRC sql
    SELECT percentile_disc(0.5) WITHIN GROUP (ORDER BY x) as median,
           percentile_cont(0.5) WITHIN GROUP (ORDER BY x) as interpolated
    FROM generate_series(0, 1) as x
    #+END_SRC

    #+RESULTS:
    | median | interpolated |
    |--------+--------------|
    |      0 |          0.5 |

*** Hypothetical aggregates

    #+BEGIN_SRC sql :exports code
    SELECT country,
           rank(9000) WITHIN GROUP (ORDER BY production DESC NULLS LAST)
    FROM t_oil
    GROUP BY ROLLUP (country);
    #+END_SRC

    #+RESULTS:
    | country              | rank |
    |----------------------+------|
    | Canada               |    1 |
    | Iran                 |    1 |
    | Iraq                 |    1 |
    | Israel               |    1 |
    | Kuwait               |    1 |
    | Mexico               |    1 |
    | Oman                 |    1 |
    | Other Middle East    |    1 |
    | Qatar                |    1 |
    | Saudi Arabien        |   21 |
    | Syria                |    1 |
    | United Arab Emirates |    1 |
    | USA                  |   27 |
    | Yemen                |    1 |
    |                      |   47 |

*** Windowing queries
**** Partitioning data


     #+BEGIN_SRC sql
     SELECT distinct(year < 1990, avg(production) OVER (PARTITION BY year < 1990))
     FROM t_oil
     #+END_SRC

     #+RESULTS:
     | row                       |
     |---------------------------|
     | (f,2801.1831501831501832) |
     | (t,2430.6856187290969900) |

     Better with a filter condition (the query plan is much less complex, and
     the query is more efficient)

     #+BEGIN_SRC sql
     SELECT
         avg(production) FILTER (WHERE year < 1990) as old,
         avg(production) FILTER (WHERE year >= 1990) as new
     FROM t_oil
     #+END_SRC

     #+RESULTS:
     |                   old |                   new |
     |-----------------------+-----------------------|
     | 2430.6856187290969900 | 2801.1831501831501832 |


     - the number of rows returned doesn't change (unlike with GROUP BY)
     - ordering within a partition matters for aggregation

    #+BEGIN_SRC sql :exports code
    SELECT country,
           year,
           production,
           consumption,
           avg(production) OVER (PARTITION BY country)
    FROM t_oil
    LIMIT 10;
    #+END_SRC

    #+RESULTS:
    | country | year | production | consumption |                   avg |
    |---------+------+------------+-------------+-----------------------|
    | Canada  | 1965 |        920 |        1108 | 2123.2173913043478261 |
    | Canada  | 2010 |       3332 |        2316 | 2123.2173913043478261 |
    | Canada  | 2009 |       3202 |        2190 | 2123.2173913043478261 |
    | Canada  | 2008 |       3207 |        2315 | 2123.2173913043478261 |
    | Canada  | 2007 |       3290 |        2361 | 2123.2173913043478261 |
    | Canada  | 2006 |       3208 |        2295 | 2123.2173913043478261 |
    | Canada  | 2005 |       3040 |        2288 | 2123.2173913043478261 |
    | Canada  | 2004 |       3079 |        2309 | 2123.2173913043478261 |
    | Canada  | 2003 |       3003 |        2228 | 2123.2173913043478261 |
    | Canada  | 2002 |       2858 |        2172 | 2123.2173913043478261 |

    #+BEGIN_SRC sql :exports code
    SELECT country,
           year,
           production,
           min(production) OVER (),
           min(production) OVER (ORDER BY year)
    FROM t_oil
    WHERE year BETWEEN 1978 AND 1983
          AND country = 'Iran';
    #+END_SRC

**** Using sliding windows

     This is a clear query to show the results of sliding windows.

     #+BEGIN_SRC sql :exports code
     SELECT *,
            array_agg(id) OVER (ORDER BY id ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)
     FROM generate_series(1, 5) as id;
     #+END_SRC

     #+RESULTS:
     | id | array_agg |
     |----+-----------|
     |  1 | {1,2}     |
     |  2 | {1,2,3}   |
     |  3 | {2,3,4}   |
     |  4 | {3,4,5}   |
     |  5 | {4,5}     |

     - Can be unbounded on both sides by UNBOUNDED FOLLOWING, or UNBOUNDED PRECEDING

**** Abstracting window clauses
     You can name your window clauses using a WINDOW clause, as in the query
     below

     #+BEGIN_SRC sql
     SELECT region, country,
            year,
            production,
            min(production) OVER (w),
            max(production) OVER (w)
     FROM t_oil
     WHERE region = 'North America' AND year BETWEEN 1980 AND 1985
     WINDOW w AS (PARTITION BY country ORDER BY year)
     #+END_SRC

     #+RESULTS:
     | region        | country | year | production |   min |   max |
     |---------------+---------+------+------------+-------+-------|
     | North America | Canada  | 1980 |       1764 |  1764 |  1764 |
     | North America | Canada  | 1981 |       1610 |  1610 |  1764 |
     | North America | Canada  | 1982 |       1590 |  1590 |  1764 |
     | North America | Canada  | 1983 |       1661 |  1590 |  1764 |
     | North America | Canada  | 1984 |       1775 |  1590 |  1775 |
     | North America | Canada  | 1985 |       1812 |  1590 |  1812 |
     | North America | Mexico  | 1980 |       2129 |  2129 |  2129 |
     | North America | Mexico  | 1981 |       2553 |  2129 |  2553 |
     | North America | Mexico  | 1982 |       3001 |  2129 |  3001 |
     | North America | Mexico  | 1983 |       2930 |  2129 |  3001 |
     | North America | Mexico  | 1984 |       2942 |  2129 |  3001 |
     | North America | Mexico  | 1985 |       2912 |  2129 |  3001 |
     | North America | USA     | 1980 |      10170 | 10170 | 10170 |
     | North America | USA     | 1981 |      10181 | 10170 | 10181 |
     | North America | USA     | 1982 |      10199 | 10170 | 10199 |
     | North America | USA     | 1983 |      10247 | 10170 | 10247 |
     | North America | USA     | 1984 |      10509 | 10170 | 10509 |
     | North America | USA     | 1985 |      10580 | 10170 | 10580 |

**** Various functions
     Windowing works with all aggregate functions, and additionaly:
     - rank
     - dense_rank
     - ntile
     - lead
     - lag
     - first_value
     - nth_value
     - last_value
     - row_number
       
     #+BEGIN_SRC sql
     SELECT year, production,
            ntile(4) OVER (ORDER BY production)
     FROM t_oil
     WHERE country = 'Iraq' AND year BETWEEN 2000 AND 2006
     #+END_SRC

     #+RESULTS:
     | year | production | ntile |
     |------+------------+-------|
     | 2003 |       1344 |     1 |
     | 2005 |       1833 |     1 |
     | 2006 |       1999 |     2 |
     | 2004 |       2030 |     2 |
     | 2002 |       2116 |     3 |
     | 2001 |       2522 |     3 |
     | 2000 |       2613 |     4 |

     #+BEGIN_SRC sql
     SELECT region, country, year, production,
            rank() OVER (PARTITION BY region ORDER BY production DESC NULLS LAST)
     FROM t_oil
     WHERE year = 2010
     ORDER BY region, rank
     #+END_SRC

     #+RESULTS:
     | region        | country              | year | production | rank |
     |---------------+----------------------+------+------------+------|
     | Middle East   | Saudi Arabien        | 2010 |      10007 |    1 |
     | Middle East   | Iran                 | 2010 |       4352 |    2 |
     | Middle East   | United Arab Emirates | 2010 |       2895 |    3 |
     | Middle East   | Kuwait               | 2010 |       2562 |    4 |
     | Middle East   | Iraq                 | 2010 |       2490 |    5 |
     | Middle East   | Qatar                | 2010 |       1655 |    6 |
     | Middle East   | Oman                 | 2010 |        865 |    7 |
     | Middle East   | Syria                | 2010 |        385 |    8 |
     | Middle East   | Yemen                | 2010 |        306 |    9 |
     | Middle East   | Other Middle East    | 2010 |        192 |   10 |
     | Middle East   | Israel               | 2010 |            |   11 |
     | North America | USA                  | 2010 |       7513 |    1 |
     | North America | Canada               | 2010 |       3332 |    2 |
     | North America | Mexico               | 2010 |       2959 |    3 |


*** Writing your own aggregates
    Writing aggregates is not hard and it can be highly beneficial to perform
    more complex operations. In this section the plan is to write a hypothetical
    aggregate, which has already been discussed in this chapter (p.120)

    #+BEGIN_SRC sql :exports code
    CREATE FUNCTION taxi_per_line (numeric, numeric)
    RETURN numeric AS
    $$
    BEGIN
    RAISE NOTICE 'intermediate: %, per row: %', $1, $2;
    RETURN $1 + $2 * 2.2;
    END;
    $$ LANGUAGE 'plpgsql';
    #+END_SRC

    #+RESULTS:

    #+BEGIN_SRC sql :exports code
    CREATE AGGREGATE taxi_price (numeric)
    (
        INITCOND = 2.5,
        SFUNC = taxi_per_line,
        STYPE = numeric
    );
    #+END_SRC

    #+RESULTS    

    One can optimize the aggregate functions to be more efficient when using
    with sliding windows. Think recursive filters. How to starts at page 118.

*** Random experiments

    #+BEGIN_SRC sql
    SELECT *
    FROM pg_catalog.pg_tables
    WHERE tablename = 't_oil';
    #+END_SRC

    #+RESULTS:
    | schemaname | tablename | tableowner | tablespace | hasindexes | hasrules | hastriggers | rowsecurity |
    |------------+-----------+------------+------------+------------+----------+-------------+-------------|
    | public     | t_oil     | bart       |            | f          | f        | f           | f           |

    #+BEGIN_SRC sql
    CREATE INDEX region_country_idx
    ON t_oil (region, country);
    #+END_SRC

    #+RESULTS:
    | CREATE INDEX |
    |--------------|


    #+BEGIN_SRC sql
    SELECT region,
           country,
           sum(production) as production,
           sum(consumption) as consumption
    FROM t_oil
    WHERE country IN ('USA', 'Canada', 'Iran', 'Oman')
    GROUP BY ROLLUP (region, country);
    #+END_SRC

    #+RESULTS:
    | region        | country | production | consumption |
    |---------------+---------+------------+-------------|
    | Middle East   | Iran    |     167058 |       44894 |
    | Middle East   | Oman    |      25804 |             |
    | Middle East   |         |     192862 |       44894 |
    | North America | Canada  |      97668 |       82728 |
    | North America | USA     |     420502 |      794365 |
    | North America |         |     518170 |      877093 |
    |               |         |     711032 |      921987 |

** Log files and system statistics
** DONE Optimizing queries for good performance
   CLOSED: [2018-01-02 Tue 15:15]
*** Optimization strategies
    - constant folding
    - view inlining
    - join reordering
    - flattening subselects
    - join pruning
    - applying equality constraints
    - function inlining
    - distribute over set operations (UNION [ALL], etc.)
    
    It is not difficult to make the process fail (e.g. by specifying OFFSET =
    0). Always run explain on a query.
*** Preliminaries
    Taken from [1].

*** Relevant system catalogs
    pg_class catalogs tables and most everything else that has columns or is
      otherwise similar to a table.
    - pg_stats is a view on top of pg_statistics
    - pg_statistics stores statistical data about the contents of the database
    - pg_stat_user_tables contains one row for each table in the current database,
      showing statistics about accesses to that specific table

*** Node types in a query plan
**** Scans
***** Index-Only Scan
      - Only needs to fetch index pages
      - Requires data to be fetched to be available from the index
      - MVCC visibility information is not stored in the index, but the table's
        visibility map has a flag for each heap page that indicates when an
        entire page is old enough to be visible to all current and future
        transactions. (see [2], Chapter 11.11 Index-Only Scans)
***** Index Scan
      - Rows are fetched in index order from the index, and then separately
        retrieved from the heap
***** Seq Scan
      - Entire table is scanned
***** Bitmap Heap Scan
      - Used after a Bitmap Index Scan, retrieves the pages selected by the Bitmap Index Scan
      - Needs to apply the filter condition again, since rows in the heap page
        fetched might not satisfy it.
***** Bitmap Index Scan
      - Gathers the pages of the rows to retrieved from the index
***** Function Scan
**** Joins
***** Hash Join
      - The rows of one of the tables are collected in a hash table (which one is indicated by Hash)
      - These rows are then looked up from the row set of the other table
***** Merge Join
      - Requires the tables to be sorted on fields in the join condition
      - Merging then takes time proportional to the number sum of the rows of
        the tables to merge.
***** Nested Loop
      - A nested loop takes time proportional to the products of the number of
        rows to merge.
**** Miscellaneous
***** Append
      - Appends to result sets
***** Unique
      - Filter out duplicates
      - Can be expensive (see [0], p. 163)
***** Sort
      - Sort the result set
****** external sort Disk
****** quicksort Memory
****** top-N heapsort Memory
       - To only provide top-n rows
***** Limit
      - Limits the result set
***** Subquery Scan

**** Aggregates
***** HashAggregate
      - Aggregate by building an in-memory hash table
***** GroupAggregate
      - Requires sorted data
      - Takes linear time, but can emit partial results

*** Understanding execution plans: Spotting problems

    Some relevant quotes from the PostgreSQL manual:

    The most critical part of the display is the estimated statement execution
    cost, which is the planner's guess at how long it will take to run the
    statement (measured in cost units that are arbitrary, but conventionally
    mean disk page fetches). Actually two numbers are shown: the start-up cost
    before the first row can be returned, and the total cost to return all the
    rows.

    The ANALYZE option causes the statement to be actually executed, not only
    planned. Then actual run time statistics are added to the display, including
    the total elapsed time expended within each plan node (in milliseconds) and
    the total number of rows it actually returned. This is useful for seeing
    whether the planner's estimates are close to reality.



    - Start where the query times jump
    - Inspect estimates
      - Maybe row sizes are over- or under-estimated due to wrong statistics
      - Maybe cross-column correlations make the estimates off (statistics in
        PostgreSQL 9.6 are univariate).
    - Inspect buffer usage

*** Miscellaneous notes

**** CLUSTER clauses
     - Rewrite the table in the same order as a (B-tree) index ([0],
       p. 170). Requires a table lock.

**** Inner joins may be reordered
     - Outer joins cannot always be reordered
     - This is probably a restatement of the algebraic properties of both of
       these types of joins in the relational algebra.

**** GROUP BY 1
     - It is possible to specify only the indices of the column to group or
       order by.

*** Partitioning data

**** Modifying inherited structure
     - Adding and removing columns propagates to the child tables
     - Adding indexes *does not*
     - It is also simple to change the parent of the child table. Maybe for
       moving data from active to history.

*** Adjusting parameters

**** work_mem
     - Query plans obviously depend on working memory.

**** maintenance_work_mem
     - Memory available for maintenance work (creating indices, etc.). Not so
       useful, maybe for creating indices on the fly.
    
** DONE Writing stored procedures
   CLOSED: [2018-01-02 Tue 15:15]
*** Takeaways
    - Probably better to use the jv8 extension that allows for using JavaScript
      in PostgreSQL as a trusted language. Also pglpsql seems quite simple.
**** Triggers are useful and flexible
     - They run in alphabetical order!
**** Types of functions
     - volatile: no assumptions on return value
     - stable: referentially transparent within a transaction
     - immutable: referentially transparent
**** PL/pgSQL is simple and takes care of more things
     - For example, caching execution plans (see [0], p.228).
**** Can create your own operators, type casts, and even collations
     - Collation is combining data, but I think refers mostly to sort orders in
       this context.
*** JavaScript is also available as trusted language
    See [3] and [4] for the full matrix of available programming languages.
** Managing PostgreSQL security
** Handling backup and recovery
** Making sense of backups and replication
** Deciding on useful extensions
** Troubleshooting PostgreSQL
** Migrating to PostgreSQL
* Extras
  :PROPERTIES:
  :engine:   postgresql
  :cmdline: "-U bart"
  :exports: both
  :visibility: folded
  :END:
  Benchmarking partitioned versus unpartitioned tables
** Partitioned

   #+BEGIN_SRC sql
   drop table p_stats cascade;

   create table p_stats (
     adset_id integer not null,
     hour integer not null,
     views integer not null,
     primary key (adset_id, hour, views)
   );


   create table p_stats_0 () inherits (p_stats);
   create table p_stats_1 () inherits (p_stats);
   create table p_stats_2 () inherits (p_stats);
   create table p_stats_3 () inherits (p_stats);
   create table p_stats_4 () inherits (p_stats);
   create table p_stats_5 () inherits (p_stats);
   create table p_stats_6 () inherits (p_stats);
   create table p_stats_7 () inherits (p_stats);
   create table p_stats_8 () inherits (p_stats);
   create table p_stats_9 () inherits (p_stats);


   alter table p_stats_0 add check (hour < 1000000);
   alter table p_stats_1 add check (hour >= 1000000 and hour < 2000000);
   alter table p_stats_2 add check (hour >= 2000000 and hour < 3000000);
   alter table p_stats_3 add check (hour >= 3000000 and hour < 4000000);
   alter table p_stats_4 add check (hour >= 4000000 and hour < 5000000);
   alter table p_stats_5 add check (hour >= 5000000 and hour < 6000000);
   alter table p_stats_6 add check (hour >= 6000000 and hour < 7000000);
   alter table p_stats_7 add check (hour >= 7000000 and hour < 8000000);
   alter table p_stats_8 add check (hour >= 8000000 and hour < 9000000);
   alter table p_stats_9 add check (hour >= 9000000 and hour < 10000000);


   insert into p_stats_0 (adset_id, hour, views)
   select 1, s, s % 100
   from generate_series(0, 999999) s;

   insert into p_stats_1 (adset_id, hour, views)
   select 1, s, s % 100
   from generate_series(1000000, 1999999) s;


   insert into p_stats_2 (adset_id, hour, views)
   select 1, s, s % 100
   from generate_series(2000000, 2999999) s;


   insert into p_stats_3 (adset_id, hour, views)
   select 1, s, s % 100
   from generate_series(3000000, 3999999) s;


   insert into p_stats_4 (adset_id, hour, views)
   select 1, s, s % 100
   from generate_series(4000000, 4999999) s;


   insert into p_stats_5 (adset_id, hour, views)
   select 1, s, s % 100
   from generate_series(5000000, 5999999) s;


   insert into p_stats_6 (adset_id, hour, views)
   select 1, s, s % 100
   from generate_series(6000000, 6999999) s;


   insert into p_stats_7 (adset_id, hour, views)
   select 1, s, s % 100
   from generate_series(7000000, 7999999) s;


   insert into p_stats_8 (adset_id, hour, views)
   select 1, s, s % 100
   from generate_series(8000000, 8999999) s;


   insert into p_stats_9 (adset_id, hour, views)
   select 1, s, s % 100
   from generate_series(9000000, 9999999) s;




   #+END_SRC


** Non-partitioned
   #+BEGIN_SRC sql
   drop table stats;

   create table stats (
     adset_id integer not null,
     hour integer not null,
     views integer not null,
     primary key (adset_id, hour, views)
   );

   insert into stats (adset_id, hour, views)
   select 1, s, s % 100
   from generate_series(1, 10000000) s;

   #+END_SRC

* References
[0] Schonig - Mastering PostgreSQL 9.6
[1] https://www.postgresql.org/docs/9.6/static/using-explain.html
[2] https://www.postgresql.org/docs/9.6/
[3] https://github.com/plv8/plv8
[4] https://wiki.postgresql.org/wiki/PL_Matrix
[5] Lehman, Yao. Efficient locking for concurrent operations on
B-trees (1981). https://www.csd.uoc.gr/~hy460/pdf/p650-lehman.pdf
[6] Fairly useful summary of the PostgreSQL cost
model. http://shiroyasha.io/the-postgresql-query-cost-model.html
