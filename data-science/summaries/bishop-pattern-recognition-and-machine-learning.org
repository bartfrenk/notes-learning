# -*- org-export-babel-evaluate: nil -*-
#+TITLE: Pattern recognition and machine learning
#+AUTHOR: Bart Frenk

#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{paralist}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{palatino}
#+LATEX_HEADER: \usepackage{euler}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \frenchspacing
#+LATEX_HEADER: \sloppy
#+LATEX_HEADER: \renewcommand{\em}[1]{\textbf{#1}}
#+LATEX_HEADER: \newcommand{\E}[1]{\operatorname{\mathbb{E}}[#1]}
#+LATEX_HEADER: \newcommand{\EE}{\mathbb{E}}
#+LATEX_HEADER: \setstretch{1.1}
#+LATEX_HEADER: \let\itemize\compactitem
#+LATEX_HEADER: \let\description\compactdesc
#+LATEX_HEADER: \let\enumerate\compactenum
#+LATEX_HEADER: \setlength{\parindent}{0em}
#+LATEX_HEADER: \setlength{\parskip}{1em}
#+LATEX_HEADER: \newcommand{\RR}{\mathbb{R}}
#+LATEX_HEADER: \newenvironment{exercise}{\textbf{Exercise.}}{}
#+OPTIONS: toc:nil todo:nil


* Connect to Jupyter                                               :noexport:
This is necessary due to [[https://github.com/gregsexton/ob-ipython/issues/141][this issue]] with =ob-ipython=.

** Set up connection
Start =jupyter console= in an appropriate directory (e.g., one which works with
a miniconda environment). This creates a =kernel-<xxxx>.json= file in the
directory below.

List all active kernels.
#+BEGIN_SRC sh
ls /run/user/1000/jupyter
#+END_SRC

#+RESULTS:
: kernel-ob.json

#+BEGIN_SRC sh
mv /run/user/1000/jupyter/kernel-30273.json /run/user/1000/jupyter/kernel-ob.json
#+END_SRC

#+RESULTS:



Create directory to store temporary (image) files:
#+BEGIN_SRC sh
mkdir -p /tmp/bishop
#+END_SRC

#+RESULTS:

Rename the =kernel-<xxxx>.json= file to =kernel-ob.json= and set the =:session=
field to that filename.
#+BEGIN_SRC ipython :session kernel-ob.json :exports code
import sys
sys.version
#+END_SRC

#+RESULTS:
: # Out[1]:
: : '3.5.2 (default, Nov 23 2017, 16:37:01) \n[GCC 5.4.0 20160609]'

** Test plotting functionality

Set up plotting configuration appropriate for use in this org mode file.
#+BEGIN_SRC ipython :session kernel-ob.json :exports code :results raw drawer
from matplotlib import rcParams
import seaborn as sns

sns.set()
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

(w, h) = rcParams['figure.figsize']
rcParams['figure.figsize'] = (1.5 * w, 1.5 * h)

rcParams['figure.facecolor'] = 'white'
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[4]:
:END:

Test if the required dependencies exist and if plotting works
#+NAME: example-plot
#+BEGIN_SRC ipython :session kernel-ob.json :exports code :ipyfile /tmp/bishop/test-hist.svg :results raw drawer
import numpy as np
import matplotlib.pyplot as plt

ys = np.random.normal(size=100)
plt.hist(ys, bins=20);
#+END_SRC

#+RESULTS: example-plot
:RESULTS:
# Out[5]:
[[file:/tmp/bishop/test-hist.svg]]
:END:

You can refer to results in the same session.
#+BEGIN_SRC ipython :session kernel-ob.json :exports code
len(ys)
#+END_SRC

#+RESULTS:
: # Out[6]:
: : 100

* Chapters
** Introduction
** Probability distributions
** DONE Linear models for regression
CLOSED: [2018-04-03 Tue 00:13]
** DONE Linear models for classification
CLOSED: [2018-04-11 Wed 22:37]
** Neural networks
** TODO Kernel methods
** Sparse kernel machines
** DONE Graphical models
CLOSED: [2018-04-03 Tue 00:13]
** DONE Mixture models and EM
CLOSED: [2018-09-16 Sun 23:39]
Given a sample $\mathcal{X}$ drawn from a probability distribution
$P(x\,|\,\theta)$. The expectation-maximization algorithm constructs a sequence of
estimators that, under certain conditions not precisely stated in the book,
converges to a maximum likelihood estimator for $\theta$ (character of the
convergence is also not stated in the book). This is an interesting topic to
really understand in detail.

The algorithm assumes the existence of a latent variable $Z$ such that a certain
optimization problem in $\theta$ over the complete data distribution $p(X,
Z\,|\,\theta)$ is tractable. It is stated as a process that computes an improved
estimate $\theta_{i + 1}$ from a given estimate $\theta_i$.

\begin{description}

\item[Expectation] Compute $q^*(z) = P(z\,|\,X, \theta_0)$.
\item[Maximization] Compute $\theta_{i + 1}$ that maximizes
$\theta \mapsto \EE_{q^*}(\ln P(\mathcal{X}, z\,|\,\theta)$. Note that
$q^*$ depends on $\theta_i$.

\end{description}

The relation to the maximum likelihood estimator is seen by considering the
additive decomposition $P(X\,|\,\theta) = \mathcal{L}(q, \theta) +
\mathrm{KL}(q\,||\,p(z\,|\,X, \theta))$, where $q$ is any probability
distribution for $Z$, and $\mathcal{L}$ is given by expression (9.71).

** WAIT Approximate inference
*** Variational inference

#+begin_exercise
Find $\alpha$ such that the uniform distribution on $[0, \alpha]$ has smallest
Kullback-Leibler divergence from the exponential distribution with parameter
$\lambda = 1$.
#+end_exercise

Compute the integral (10.3), to end up with a KL divergence of $-\ln(\alpha) +
\frac{1}{2} \alpha$.

#+begin_src gnuplot :file kullback-leibler.png
plot [0:10] -log(x) + x / 2
#+end_src

#+RESULTS:
[[file:kullback-leibler.png]]

*** Digression: distances between probability measures
- Hellinger distance (member of the alpha family of divergences)
- Wasserstein distance (apparently popular for generative adversarial networks
  (see [[https://datascience.stackexchange.com/questions/22725/what-is-hellinger-distance-and-when-to-use-it][Stack Exchange]]).

** DONE Sampling methods
CLOSED: [2018-04-03 Tue 00:13]
** Continuous latent variables
** Sequential data
** Combining models
