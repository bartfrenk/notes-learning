#+TITLE: Notes on A tutorial on Thompson sampling
#+AUTHOR: Bart Frenk

#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{paralist}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{palatino}
#+LATEX_HEADER: \usepackage{euler}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \renewcommand{\em}[1]{\textbf{#1}}
#+LATEX_HEADER: \newcommand{\E}[1]{\operatorname{\mathbb{E}}[#1]}
#+LATEX_HEADER: \setstretch{1.1}
#+LATEX_HEADER: \let\itemize\compactitem
#+LATEX_HEADER: \let\description\compactdesc
#+LATEX_HEADER: \let\enumerate\compactenum
#+LATEX_HEADER: \setlength{\parindent}{0em}
#+LATEX_HEADER: \setlength{\parskip}{1em}
#+LATEX_HEADER: \newcommand{\RR}{\mathbb{R}}
#+OPTIONS: toc:nil todo:nil


* Tutorial on Thompson sampling
** DONE Introduction
CLOSED: [2018-03-20 Tue 21:28]
** DONE Greedy decisions
CLOSED: [2018-03-20 Tue 21:28]
** DONE Thompson sampling for the Bernoulli bandit
CLOSED: [2018-03-20 Tue 21:28]
** DONE General Thompson sampling
CLOSED: [2018-03-20 Tue 21:28]
** CANCELED Approximations
CLOSED: [2018-08-21 Tue 21:03]

Approximations are not so useful, since computation time grows with the size of
the size of the history.

In order to keep the computational burden manageable, it can be important to
consider incremental variants of our approximation methods. We refer to an
algorithm as *incremental* if it operates with *fixed* rather than growing
per-period compute time. There are many ways to design incremental variants of
approximate posterior sampling algorithms we have presented. As concrete
examples, we consider here particular incremental versions of Laplace
approximation and bootstrap approaches. (p.19)

** Practical modeling considerations
** Further examples
** Why it work, when it fails, and alternative approaches


* CANCELED An empirical evaluation of Thompson sampling
CLOSED: [2018-08-21 Tue 21:03]
** About

Article published in *december 2011*.

Has the example of optimizing the CTR of an ad display campaign. They use a
Laplace approximation of the posterior, and a logistic regression model to
relate context to CTRs. Very similar to what we might do. Has quite some
references to articles dealing with display ad campaigns.

Authors are Olivier Chapelle and Lihong Li, both from Yahoo! Research.

** Notes

*** Introduction

#+BEGIN_QUOTE
In this work, we present some empirical results, first on a simulated problem
and then on two real-world ones: display advertisement selection and news
article recommendation. In all cases, despite its simplicity, Thompson sampling
achieves state-of-the-art results, and in some cases significantly outperforms
other alternatives like UCB. The findings suggest the necessity to include
Thompson sampling as part of the standard baselines to compare against, and to
develop finite-time regret bound for this empirically successful
algorithm. (p.1)
#+END_QUOTE

*** Algorithms

- Upper confidence bound (UCB) :: Strong theoretical guarantees on the
     regret. There are various variants of the UCB algorithm, but they all have
     in common that the confidence parameter should increase over time. (p.2)
- Bayes-optimal approach of Gittins :: Directly maximizes expected cumulative
  payoffs with respect to a given prior distribution. 
- Probability matching (Thompson sampling) :: Heuristic (*from 1933*)

Write
\[
Q(a, x, r) = \mathbb{I}\Big(\E{r \mid a, x, \theta} = max_{a'} \E{r \mid a', x, \theta}\Big)
\]

I find it easier to understand the expression

\[
\int Q(a, x, r) \, P(\theta \mid D) d\theta
\]

by considering the (hypothetical) case in which there is exactly one $\theta'$
such that $Q(a, x, r)$ is $1$. The expression then reduces to $P(\theta' | D)$,
which is exactly the probability that $a$ is the action with the expected
maximal reward.

*** Simulations

#+BEGIN_QUOTE
We can thus conclude that in these simulations, Thompson sampling is
asymptotically optimal and achieves a smaller regret than the popular UCB
algorithm. It is important to note that for UCB, the confidence bound (1) is
tight; we have tried some other confidence bounds, including the one originally
proposed in [3], but they resulted in larger regrets. (p.4)
#+END_QUOTE

*Optimistic Thompson selecting*

For an action $a$:

1. Draw $\theta'$ from the posterior.
2. Compute the expected reward for $a$ conditional on $\theta'$.
3. Compute the expected reward for $a$, unconditionally. 
4. Take the maximum of step 2. and 3.

This results in a marginally better regret in the simulations. The difference is
small.

*Posterior reshaping*

Widen the variance, by having a posterior with parameters $a/\alpha$ and $b / \alpha$.

*** Display advertising

#+BEGIN_QUOTE
In this paper, we consider standard regularized logistic regression for
predicting CTR. There are several features representing the user, page, ad, as
well as conjunctions of these features. Some of the features include identifiers
of the ad, advertiser, publisher and visited page. These features are
hashed and each training sample ends up being represented as sparse binary
vector of dimension $2^{24}$. (p.5)
#+END_QUOTE

They use the Laplace approximation of the posterior, in the following sense:
1. The Laplace approximation of the posterior is used as an approximation to the
   prior in the next Bayesian update step.
2. Instead of sampling from the posterior, they sample from the Laplace
   approximation to the posterior.

I think they use some type of per-period regret, since the regret is decreasing
over time. Could not find how long the period was.

Note that clicks are simulated based on weight parameters estimated from real
data.

They have 13000 contexts per hour; the number of eligible ads varies between
5,910 and 1, with a mean of 1,364 and a median of 514.

#+BEGIN_SRC python :exports both
return 13000 / 3600
#+END_SRC

I think they use a single logistic regression model, with input features derived
from a (context, ad) pair, i.e.,

\[
\E{Y|X=x, A=a} = \sigma(h(x, a))
\]

**** DONE Look into feature hashing
CLOSED: [2018-05-17 Thu 00:36]

Start here: Why feature hashing? They link to an article [3].

*** News article recommendation

#+BEGIN_QUOTE
Each time a user visits the portal, a news article out of a small pool of
hand-picked candidates is recommended. The *candidate pool is dynamic*: old
articles may retire and new articles may be added in. The average size of the
pool is around 20.  The goal is to choose the most interesting article to users,
or formally, maximize the total number of clicks on the recommended articles. In
this case, we treat articles as arms, and define the payoff to be 1 if the
article is clicked on and 0 otherwise. Therefore, the average per-trial payoff
of a policy is its overall CTR.(p. 7, emphasis mine)
#+END_QUOTE


Project large sparse user feature vector on the first 20 principal
components. This makes learning easier, and more computationally cheaper.

Use a logistic regression model for each article.

The authors mention this reason.

Indeed, it is shown in our previous work (Figure 5) that
article features are helpful in this domain only when data are highly sparse.

and refer to [4], of which one of the authors is a co-author.

* Extra

** Bounds
Mentioned in *An emperical evaluation of Thompson sampling*.

- Chernoff bound ::
- Markov inequality ::
- Chebyshev bound ::

** Blogpost on how to deal with more complicated models
https://eigenfoo.xyz/bayesian-bandits/.

Interesting that it is possible to use the Bernoulli model to deal with more
complicated ones. The article is mentioned on the post, but here is the link:
https://arxiv.org/abs/1111.1797. Written by two people from Microsoft Research
India.

Other interesting links from the blog post:
- http://banditalgs.com/ (free Cambridge University Press book on bandit
  algorithms). The pdf is [[http://downloads.tor-lattimore.com/banditbook/book.pdf][here]].


* Time sensitive bandit learning /
Might be useful to read. Deals with the situation in which finding a
sufficiently in good action in limited time is the main criterium. Probably good
for practical cases.

* Footnotes

[1] Chapelle ea. An empirical evaluation of Thompson sampling
[2] William R. Thompson. On the likelihood that one unknown probabilily exceeds
another in view of the evidence of two samples (1933).
[3] Weinberger, ea. Feature hashing for large scale multitask learning.
[4] L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach
to personalized news article recommendation. In Proceedings of the 19th
international conference on World wide web, pages 661â€“670, 2010.
[5] Russo, ea. Time sensitive bandit learning and satisficing Thompson sampling. (2017)
