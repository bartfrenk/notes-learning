#+TITLE: Notes on Practical Deep Learning For Coders (fast.ai)
#+AUTHOR: Bart Frenk

* Information

* Lessons
** DONE Recognizing cats and dogs
   CLOSED: [2018-02-07 Wed 13:40]
*** DONE Set up paperspace account and rent GPU (0.40$/hour)
    CLOSED: [2018-02-07 Wed 11:25]
    *Remember to close SSH connections and stop machine!*

*** DONE Read Wikipedia article on Arthur Samuel    
    CLOSED: [2018-02-07 Wed 13:42]
    Machine learning invented by [[https://en.wikipedia.org/wiki/Arthur_Samuel][Arthur Samuel]]. He coined the term in 1959.

*** Ingredients
**** Infinitely flexible function (parametrizable)
     - Neural network
     - Universal approximation theorem
**** All-purpose parameters fitting
     - Gradient descent
     - Problem of local optima is not so relevant for neural networks (why?)
**** Fast and scalable
     - GPUs (cheaper than CPUs and faster for the problem at hand)
     - Universal approximation for a single hidden layers requires an
       exponentially increasing number of parameters. Solved by introducing
       multiple hidden layers.

  No need for intensive and domain-specific feature engineering.

*** Projects
**** Many at Google (exponential growth)
**** Microsoft Skype Translator Preview
     [3]
**** Semantic style transfer
     Turn doodles into paintings in a specific style
     (impressionist) [4]. Creative AI.
**** Diagnose lung cancer
**** Many more:
     ao. Ad optimization, pricing, resume screening

*** How does it work?
    Interesting link, convolutions explained visually [5]

**** Gradient descent
**** Learning rate

**** Interesting paper
     Visualizing and Understanding Convolutional Networks (2013) [6]
     Draw a picture of what each layers in a convolutional neural networks learned.

**** Choosing a learning rate
     Cyclical Learning Rates for Training Neural Networks (2015) [7]

**** DONE For this week: experiment
     CLOSED: [2018-02-10 Sat 21:41]
     - What image does it work well for
     - How does the learning rate vary
     - How many epochs
     - Get a sense for what is in the data object
     - Practice with numpy
***** Learn three or four keyboard shortcuts per day

**** Tricks in Jupyter notebook
     - Tab completion method names
     - Shift-Tab to get the signature and docstring, twice to get documentation,
       thrice to make help appear in a separate frame
     - ?<method name> also brings docs up the separate frame
     - ??<method name> brings up the source code
     - press 'h' brings up keyboard shortcuts (learn three or four keyboard
       shortcuts per day)

** DONE Improving your image classifier
   CLOSED: [2018-02-23 Fri 11:20]
*** Learning rate 
    If your loss if going off to infinity, it is probably because your learning
    rate is too high.
    
    Learning rate trainer. Important plot is the loss versus the learning
    rate. Rule of thumb: one magnitude smaller (x10) than the minimum.
*** Data augmentation
    Transform data, acceptable transforms depend on type of images, e.g. no use
    flipping letters.
*** Cycle length
    Stochastic gradient descent with restarts.

    - Learning rate annealing :: Decreasing learning rate.
    - Cosine annealing :: Take learning rate according to half of the cosine.

*** Saving learning rate
    Good to save learned parameters every once in a while.
*** Different learning rates for different layers
    Differential learning rates

*** Test time augmentation
    Augment the data in the validation set.

*** Steps to get a world-class image classifier
**** Full version
    1. enable data augmentation, and precompute=True
    2. use lr_find() to find highest learning rate where loss is still clearly improving
    3. train last layer from precomputed activations for 1-2 epochs
    4. train last layer with data augmentation (i.e. precompute=False) for 2-3 epoch with cycle_len=1
    5. unfreeze all layers
    6. set earlier layers to 3x-10x lower learning rate than next higher layer
    7. use lr_find() again
    8. train full network with cycle_mult=2 until over-fitting
**** Minimal version
    2. use lr_find() to find highest learning rate where loss is still clearly improving
    4. train last layer with data augmentation (i.e. precompute=False) for 2-3 epoch with cycle_len=1
    5. unfreeze all layers
    6. set earlier layers to 3x-10x lower learning rate than next higher layer
    8. train full network with cycle_mult=2 until over-fitting
**** Summarized
     1. find good learning rate
     2. train last layer (rest of the network is frozen)
     3. train full network with differential learning rates
*** Things that improve the classifier
    - bigger size, if the size of the images allow it
    - different architecture  
      - resnet44
      - resnext50
     
*** DONE For this week: Do the dog breed classification
    CLOSED: [2018-02-17 Sat 00:11]
    Use the course 1 notebook
** DONE Understanding convolutions
   CLOSED: [2018-03-01 Thu 22:03]
   Useful command
   #+BEGIN_SRC ipython :session :exports code
   for t in tables:
       display(t.head()) # display renders the widget
   #+END_SRC
   Also, Jeremy has collapsing sections in the Jupyter notebook.

*** DONE Try out image classification challenges from Kaggle
    CLOSED: [2018-03-16 Fri 17:27]
    Aim is to gain familiarity with the fast.ai method for image classification.
    - dog breeds
    - plan (amazon)
*** WAIT Try image classification on Kruidvat website pictures
    Aim is to predict the category.
** DONE Structured time series and language models
   CLOSED: [2018-03-09 Fri 11:14]
*** DONE Recommended blog post on setting learning rates
    CLOSED: [2018-03-03 Sat 22:31]
    https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b
    Article on different ways to improve setting the learning rate for the
    optimizer of a deep neural network.
    - How to estimate a good learning rate (plot it against loss or accuracy)
    - Cyclic learning rates
    - Snapshots ensemble. Ensemble the parameters of multiple local minima that
      are found at the end of each (later) cycle of training.
*** Transfer learning
*** Drop out
    Absolutely critical. Just about solved the problem of generalization.

*** Cross-validation for time series data
    Depends on what to predict. Usually the future. So cross-validate on the
    last data points, e.g., the validation set is the last 25% percent of the
    data points, ordered by timestamp.
**** DONE Rachel Thomas' post How (and why) to create a good validation set
     CLOSED: [2018-03-09 Fri 11:30]
***** Quotes
      - On of the most likely culprits for this disconnect between results in
        development vs results in production is a poorly chosen validation set.
      - .. takes a random subset of the data, which is a poor choice for many
        real-world problems.
      - a *key property* of the validation and test sets is that they must be
        *representative of the new data you will see in the future*.
***** Training set, validation set, test set
      - the training set is used to train a given model
      - the validation set is used to choose between models
      - the test set tells you how you've done
*** Embeddings
    Rich distributed representation
     http://www.fast.ai/2017/11/13/validation-sets/
*** TODO Predict story points from JIRA issue text
No idea if this can be done. Use the same approach as Jeremy Howard:
- train a language model on the English Wikipedia
- use this model as a basis when training on actual JIRA issues

This could greatly profit from fast AI's model zoo, once it is up:
http://nlp.fast.ai/category/model_zoo.html
  
** DONE Collaborative filtering. Inside the training loop
CLOSED: [2018-03-21 Wed 18:15]
*** DONE Do PyTorch tutorial
CLOSED: [2018-03-26 Mon 22:50]
The assignment was to play around with stuff you are least familiar with.
** DONE Interpreting embeddings. RNNs from scratch
CLOSED: [2018-03-24 Sat 18:21]
*** WAIT Look at Rossmann kernels
Recommended by Jeremy Howard, since there are a few good ones.
*** animation in matplotlib
See the regression line moving into place.
** STARTED Resnets from scratch


* References

[1] http://www.fast.ai/ (course website)
[2] https://www.paperspace.com/ (machine provider)
[3] https://www.skype.com/en/features/skype-translator/
[4] https://arxiv.org/abs/1603.01768
[5] http://setosa.io/ev/image-kernels/
[6] https://arxiv.org/abs/1311.2901
[7] https://arxiv.org/abs/1506.01186
