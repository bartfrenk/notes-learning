# -*- org-export-babel-evaluate: nil -*-
#+TITLE: Notes on hash kernels
#+AUTHOR: Bart Frenk

#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{paralist}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{palatino}
#+LATEX_HEADER: \usepackage{euler}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \renewcommand{\em}[1]{\textbf{#1}}
#+LATEX_HEADER: \newcommand{\E}[1]{\operatorname{\mathbb{E}}[#1]}
#+LATEX_HEADER: \setstretch{1.1}
#+LATEX_HEADER: \let\itemize\compactitem
#+LATEX_HEADER: \let\description\compactdesc
#+LATEX_HEADER: \let\enumerate\compactenum
#+LATEX_HEADER: \setlength{\parindent}{0em}
#+LATEX_HEADER: \setlength{\parskip}{1em}
#+LATEX_HEADER: \newcommand{\RR}{\mathbb{R}}
#+OPTIONS: toc:nil todo:nil

* Meta


1. Weinberger, Dasgupta, Langford, Smola, Attenberg. Feature hashing for large
   scale multitask learning. Proceedings of the 26th International Conference on
   Machine Learning, Montreal, Canada, 2009. (Mostly Yahoo!) 
2. Shi, Petterson, Langford, Smola, Strehl, Dror, Vishnwanathan. Hash
   kernels. 2009.

* Feature hashing for large scale multitask learning

Use a random function to map a higher dimensional space (of features) into a
lower dimensional space. The expectation of the inner product of the range
equals the inner product on the domain.

The article is sloppy with probability distributions. How to define one on the
function space $\{1, \ldots, m\} \rightarrow \mathbb{R}$?.

** Introduction
** Hash functions

Let $\mathcal{X}$ be a high dimensional feature space, and $\mathcal{Y}$ a lower
dimensional target space. Both are equipped with inner products. The aim is to
define a random variable $\Phi$ with values in the function space $\mathcal{X}
\rightarrow \mathcal{Y}$, such that
\[
x_1 \cdot x_2 = \mathbb{E}(\Phi(x_1) \cdot \Phi(x_2)).
\]
It is important that $\Phi(x_1) \cdot \Phi(x_2)$ has low variance.




* Hash kernels

* Experiments
:PROPERTIES:
:header-args: :session kernel-26595.json
:END:

** Set up connection                                              :noexport:
This is necessary due to [[https://github.com/gregsexton/ob-ipython/issues/141][this issue]] with =ob-ipython=.

Start =jupyter console= in an appropriate directory (e.g., one which works with
a miniconda environment). This creates a =kernel-<xxxx>.json= file in the
directory below.

List all active kernels.
#+BEGIN_SRC sh
ls /run/user/1000/jupyter
#+END_SRC

#+RESULTS:
: kernel-26595.json

Create directory to store temporary (image) files:
#+BEGIN_SRC sh
mkdir -p /tmp/hash-kernel
#+END_SRC

Rename the =kernel-<xxxx>.json= file to =kernel-ob.json= and set the =:session=
field to that filename.
#+BEGIN_SRC ipython
import sys
sys.version
#+END_SRC

#+RESULTS:
: # Out[2]:
: : '3.5.2 (default, Nov 23 2017, 16:37:01) \n[GCC 5.4.0 20160609]'

#+BEGIN_SRC ipython :results raw drawer
from matplotlib import rcParams
import seaborn as sns

sns.set()
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

(w, h) = rcParams['figure.figsize']
rcParams['figure.figsize'] = (1.5 * w, 1.5 * h)

rcParams['figure.facecolor'] = 'white'
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[5]:
:END:

** Unbiased kernel estimator                                      :noexport:

#+NAME: fig:estimator
#+BEGIN_SRC ipython :cache yes :ipyfile /tmp/hash-kernels/hist.png :exports results :results graphics
import numpy as np
import matplotlib.pyplot as plt

m = 100
d = 10
t = 1
s = 1

x = 10 * np.random.randn(m)
y = 10 * np.random.randn(m)

sample = []

for _ in range(s):
    total = 0
    for _ in range(t):

        xi = np.random.randint(0, 2, m) * 2 - 1
        phi = np.random.randint(0, d, m)

        x_ = xi * x
        y_ = xi * y

        v = np.array([sum(x_[phi == i]) for i in range(d)])
        w = np.array([sum(y_[phi == i]) for i in range(d)])

        total += np.dot(v, w)

    sample.append(total / t)

plt.hist(sample, bins=20);
plt.axvline(np.dot(x, y), color='r');
#+END_SRC

#+RESULTS[0a9008ee15f79a60066971b1eab3f39bd68839aa]:
:RESULTS:
# Out[35]:
[[file:/tmp/hash-kernels/hist.svg]]
:END:


